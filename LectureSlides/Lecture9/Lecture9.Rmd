---
title: 'MPP-E1180 Lecture 9: Statistical Modeling with R'
author: "Christopher Gandrud"
date: "18 November 2016"
output:
  ioslides_presentation:
    css: https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css
    logo: https://raw.githubusercontent.com/christophergandrud/Hertie_Collab_Data_Science/master/img/HertieCollaborativeDataLogo_v1.png
  beamer_presentation: default
bibliography: main.bib
---

## <i class="fa fa-arrow-circle-o-up"></i> Objectives for the class

- Assignment 3

- Review

- Intro to the general syntax for statistical modelling in R.

- Specific examples using:

    + Normal linear regression

    + Logistic regression
    
    + Panel data

## Collaborative Research Project (1)

**Purposes**: Pose an interesting research question and try to answer it using
data analysis and standard academic practices. Effectively communicate your
results to a **variety of audiences** in a **variety of formats**.

**Deadline**:

- Presentation: In-class 2 December

- Website/Paper: 16 December 2016

## Collaborative Research Project (2)

The project can be thought of as a 'dry run' for your thesis with multiple 
presentation outputs.

Presentation: 10 minutes **maximum**. **Engagingly** present your research
question and key findings to a general academic audience (fellow students).

Paper: 5,000 words maximum. **Standard academic paper**, properly cited laying out
your research question, literature review, data, methods, and findings.

Website: An engaging website designed to convey your research to **a general
audience**.

## Collaborative Research Project (3)

As always, you should **submit one GitHub repository** with all of the
materials needed to **completely reproduce** your data gathering, analysis, and
presentation documents.

**Note**: Because you've had two assignments already to work on parts of the
project, I expect **high quality work**.

## Collaborative Research Project (4)

Find one other group to be a **discussant** for your presentation.

The discussants will provide a quick (max 2 minute) critique of your 
presentation--ideas for things you can improve on your paper.

## Office hours

I will have normal office hours every week for the rest of the term 
**except 9 December**.

Please take advantages of this opportunity to **improve your final project**.

## Review

- What is **web scraping**? What are some of tools R has for web scraping?

- What are **regular expressions** (give at least two examples)? Why are they
useful?

- What dplyr function can you use to create a **new variable** in a data frame by
running a command on values from groups in that data frame?

## Statistical Modelling in R

**Caveat**: We are **definitely not** going to cover anywhere near R's full
capabilities for statistical modeling.

We are also **not going to cover** all of the **modeling concerns/diagnostics** 
you need to consider when using a given model.

You will need to **rely on your other stats courses** and **texts**.

## <i class="fa fa-question"></i> What are we going to do?

- Discuss the basic syntax and capabilities in R for estimating normal linear
and logistic regressions.

- Basic model checking in R.

- Discuss basic ways of interpreting results (we'll do more on this next week).

## The basic model

Most statistical models you will estimate are from a general class (**Generalised Linear Model**) that
has **two parts**:

**Stocastic Component** (e.g. randomly determined) assumes 
the dependent variable $Y_{i}$ is generated from as a random draw from the 
probability density function:

$$
Y_{i} \sim f(\theta_{i},\: \alpha)
$$

- $\theta_{i}$: parameter vector of the part of the function that **varies between
observations**.

- $\alpha$: matrix of **non-varying parameters**.

Sometimes referred to as the '**error structure**'.

## The basic model

The **Systematic Component** indicating how $\theta_{i}$ varies across
observations depending on values of the explanatory variables and (often)
some constant:

$$
\theta_{i} = g(X_{i},\: \beta)
$$

- $X_{i}$: a $1\: \mathrm{x}\: k$ vector of **explanatory variables**.

- $\beta$: a $1\: \mathrm{x}\: k$ vector of **parameters**
(i.e. coefficients).

- $g(.,.)$: the **link function**, specifying how the explanatory
variables and parameters are translated into $\theta_{i}$.

## Today

Today we will cover two variations of this general model:

- linear-normal regression (i.e. ordinary least squares)

- logit model

## Linear-normal regression

For continuous dependent variables assume that $Y_{i}$ is from the
 **normal distribution** ($N(.,.)$).

Set the main parameter vector $\theta_{i}$ to the **scalar mean** of:
$\theta_{i} = E(y_{i})= \mu_{i}$.

- **Scalar**: a real number (in R-language: a vector of length 1)

Assume the ancillary parameter matrix is the scalar homoskedastic variance:
$\alpha = V(Y_{i}) = \sigma^2$.

- **Homoskedastic variance**: variance does not depend on the value of $x$.
The standard deviation of the error terms is constant across values of $x$.

Set the systematic component to the linear form:
$g(X_{i},\: \beta) = X_{i}\beta = \beta_{0} + X_{i1}\beta_{1} + \ldots$.

## Linear-normal regression

<br>

So:

$$
Y_{i} \sim N(\mu_{i},\: \sigma^2), \:\:\: \mu_{i} = X_{i}\beta
$$

## Logit regression

For binary data (e.g. 0, 1) we can assume that the stochastic component has a
Bernoulli distribution.

The main parameter is $\pi_{i} = \mathrm{Pr}(Y_{i} = 1)$.

The systematic component is set to a logistic form:
$\pi_{i} = \frac{1}{1 + e^{-X_{i}\beta}}$.

So:

$$
Y_{i} \sim \mathrm{Bernoulli}(\pi_{i}), \:\:\: \pi_{i} = \frac{1}{1 + e^{-X_{i}\beta}}
$$

## Example error structure families and link functions

| Error Family | Canonical link |
| ------------ | -------------- |
| Normal       | identity       |
| binomial     | logit          |
| poisson      | log            |

## R syntax

The general syntax for estimating statistical models in R is:


```{r, eval=FALSE}
response variable ~ explanatory variable(s)
```

Where '`~`' reads 'is modelled as a function of'.

In the Generalised Linear Model context, either explicitly or implicitly:

```{r, eval=FALSE}
response variable ~ explanatory variable(s), family = error family
```

## Model functions

We use model functions to specify the model structure.

Basic model functions include:

- `lm`: fits a linear model where $Y$ is assumed to be normally distributed
and with homoskedastic variance.

- `glm`: allows the fitting of many Generalised Linear Models. Lets you specify
the error `family`.

- `plm` (package and function): panel data Linear Models 

- `pglm` (package and function): panel data Generalised Linear Models

## Example of `lm`

Example data *Prestige* (example based on
<http://www.princeton.edu/~otorres/Regression101R.pdf>).

The observations are **occupations** and the dependent variable is a score of
each occupation's **prestige**.

```{r}
library(car)
data(Prestige)
```

## Examine correlation matrix

```{r}
car::scatterplotMatrix(Prestige)
```


## Example of `lm`

Estimate simple model (education is in years):

```{r}
M1 <- lm(prestige ~ education, data = Prestige)
```

---

```{r}
summary(M1)
```

## Confidence intervals of parameter point estimates

Note: **Always prefer estimation intervals** over point estimates.

Deal with your **uncertainty**!

About **95%** of the time the population parameter will be within **about 2 
standard errors** of the point estimate. 

Using **Central Limit Theorem** (at least about 50 observations and the data is not
extremely skewed):

$$
CI\_95 = \mathrm{point\: estimate} \pm 1.96 * SE
$$

## Confidence intervals of parameter point estimates

```{r}
confint(M1)
```

## Example of `lm`

Estimate model with categorical (factor) variable:

```{r}
M2 <- lm(prestige ~ education + type, 
         data = Prestige)
```

---

```{r}
summary(M2)
```

## Create categorical variable from continuous variable

Use the `cut` function to create a categorical (factor) variable from a
continuous variable.

```{r}
Prestige$income_cat <- cut(Prestige$income,
                breaks = c(0, 4999, 9999, 14999, 30000),
                labels = c('< 5,000', '< 10,000', '< 15,000',
                            '>= 15,000'))
summary(Prestige$income_cat)
```

Note: `cut` excludes the left value and includes the right value, e.g.
$(0,\: 4999]$.

## Example of `lm`

```{r}
M3 <- lm(prestige ~ education + income_cat, 
         data = Prestige)
confint(M3)
```

## Example of `lm`

Estimate models with polynomial transformations:

```{r}
# Cubic polynomial transformation
M4  <- lm(prestige ~ education + poly(income, 2), 
          data = Prestige)
confint(M4)
```

## Example of `lm`

Estimate models with (natural) logarithmic transformations:

```{r}
# Cubic polynomial transformation
M5  <- lm(prestige ~ education + log(income), 
          data = Prestige)
```

---
```{r}
summary(M5)
```

## Example of `lm`

Estimate model with interactions:

```{r}
M6 <- lm(prestige ~ education * type, 
         data = Prestige)
```

---

```{r}
summary(M6)
```

## Diagnose heteroscedasticity

Use `plot` on a model object to run visual diagnostics.

```{r}
plot(M2, which = 1)
```

## Diagnose non-normality of errors

`plot` to see if a model's errors are normally distributed.

```{r}
plot(M2, which = 2)
```

## Example of logistic regression with `glm`

Example from [UCLA IDRE](http://www.ats.ucla.edu/stat/r/dae/logit.htm).

Simulated data of admission to grad school.

```{r}
# Load data
URL <- 'http://www.ats.ucla.edu/stat/data/binary.csv'
Admission <- read.csv(URL)
```

## Example of logistic regression with `glm`

```{r, warning=FALSE, message=FALSE}
car::scatterplotMatrix(Admission)
```

## Contingency table for school rank and admission

```{r}
admit_table <- xtabs(~admit + rank, data = Admission)
admit_table
```

## Row and column proportions

```{r}
# Row proportions
prop.table(admit_table, margin = 1)

# Column proportions
prop.table(admit_table, margin = 2)
```


## Summary of contingency table for school rank and admission

```{r}
summary(admit_table)
```

## Example of logistic regression with `glm`

```{r}
Logit1 <- glm(admit ~ gre + gpa + as.factor(rank),
              data = Admission, family = 'binomial')
```

Note: Link function is assumed to be logit if `family = 'binomial'`.

## Example of logistic regression with `glm`

```{r, message=FALSE}
confint(Logit1)
```

## Interpreting logistic regression results

$\beta$'s in logistic regression are interpretable as **log odds**. These are
weird.

If we exponentiate log odds we get **odds ratios**.

```{r, message=FALSE}
exp(cbind(OddsRatio = coef(Logit1), confint(Logit1)))
```

These are **also weird**.

## Interpreting logistic regression results

What we really want are **predicted probabilities**

**First** create a data frame of fitted values:

```{r}
fitted <- with(Admission, 
               data.frame(gre = mean(gre), 
                          gpa = mean(gpa),
                          rank = factor(1:4)))
fitted
```

## Interpreting logistic regression results

**Second** predict probability point estimates for each fitted value.

```{r}
fitted$predicted <- predict(Logit1, newdata = fitted,
                            type = 'response')

fitted
```




## Showing results from regression models with simulations

<br>
<br>
<br>

@King2001 argue that **post-estimation simulations** can be used to
effectively communicate **results from regression models**.

## Steps

1. Estimate our parameters' point estimates for $\hat{\beta}_{1\ldots k}$.

2. Draw $n$ values of the point estimates from multivariate normal distributions
with means $\bar{\beta}_{1\ldots k}$ and variances specified by the parameters'
estimated co-variance.

3. Use the simulated values to calculate quantities of interest (e.g. predicted
probabilities).

4. Plot the simulated distribution using **visual weighting**.

## Notes

Post-estimation simulations allow us to effectively communicate our estimates
and the **uncertainty around them**.

This method is broadly similar to a fully Bayesian approach with Markov-Chain
Monte Carlo or bootstrapping. Just differ on **how the parameters are
drawn**.

## Implementation

1. Find the coefficient estimates from an estimated model with `coef`.

2. Find the co-variance matrix with `vcov`.

3. Draw point estimates from the multivariate normal distribution with `mvrnorm`.

4. Calculate the quantity of interest with the draws + fitted values using
and plot the results.

## Simulations: estimate model

First estimate your model as normal and create fitted values:

```{r, message=FALSE}
library(car) # Contains data
library(dplyr) # Piping function
M_prest <- lm(prestige ~ education + type, 
         data = Prestige)

# Find a range of education values
range(Prestige$education)

edu_range <- 6:16
```

## Simulations: extract estimates

Extract point estimates (coefficients) and co-variance matrix:

```{r}
mp_coef <- matrix(coef(M_prest))
mp_vcov <- vcov(M_prest)
```

Now draw 1,000 simulations of your point estimates:

```{r message=FALSE}
library(MASS) # contains the mvrnorm function
drawn <- mvrnorm(n = 1000, mu = mp_coef, Sigma = mp_vcov) %>% 
            data.frame
head(drawn)[1:3, ]
```

## Simulations: merge in fitted values

Now we can add in our fitted values to the simulation data frame:

```{r}
drawn_sim <- merge(drawn, edu_range)

# Rename the fitted value variable
drawn_sim <- dplyr::rename(drawn_sim, fitted_edu = y)

nrow(drawn)
nrow(drawn_sim)
```


## Simulations: calculate quantity of interest

Using the normal linear regression formula ($\hat{y_{i}} = \hat{\alpha} + X_{i1}\hat{\beta_{1}} + \ldots$) 
we can find the quantity of interest for white collar workers:

```{r}
names(drawn_sim)
drawn_sim$sim_wc <- drawn_sim[, 1] + drawn_sim[, 2] * drawn_sim[, 5] + 
                        drawn_sim[, 3]
```


## Simulations: plot points

```{r include=FALSE}
library(ggplot2) # for page formatting
```

```{r, message=FALSE}
ggplot(drawn_sim, aes(fitted_edu, sim_wc)) + 
        geom_point(alpha = 0.2) + stat_smooth(se = FALSE) +
        theme_bw()

```

## Simulations: find 95% central interval ribbons

```{r}
# Find 95% central interval and median at each fitted value of edu
central <- drawn_sim %>% group_by(fitted_edu) %>%
            summarise(median_sim = median(sim_wc),
                      lower_95 = quantile(sim_wc, probs = 0.025),
                      upper_95 = quantile(sim_wc, probs = 0.975)
            )
```

## Simulations: plot 95% central interval

```{r}
ggplot(central, aes(fitted_edu, median_sim)) +
    geom_ribbon(aes(ymin = lower_95, ymax = upper_95), alpha = 0.3) +
    geom_line() + theme_bw()
```

## Predictions from logistic regression

Use the same steps for simulating predicted outcomes from logistic regression models.
The only difference is that the equation for the quantity of interest is:

$$
P(y_{i} = 1) = \frac{\mathrm{exp}(\hat{\alpha} + X_{i1}\hat{\beta_{1}} + \ldots)}{1 - \mathrm{exp}(\hat{\alpha} + X_{i1}\hat{\beta_{1}} + \ldots)}
$$


## Easier Implementation

<br>
<br>
<br>

The [Zelig](http://gking.harvard.edu/zelig) package streamlines the simulation process.

## Zelig (1)

First estimate your regression model using `zelig`.

```{r, message=FALSE}
library(Zelig)

# Have to explicitly declare rank as factor
Admission$rank <- as.factor(Admission$rank)

Z1 <- zelig(admit ~ gre + gpa + rank, cite = FALSE,
              data = Admission, model = 'logit')
```

## Zelig (2)

Then set the fitted values with `setx`.

```{r}
setZ1 <- setx(Z1, gre = 220:800)
```

And run the simulations (1,000 by default) with `sim`.

```{r}
simZ1 <- sim(Z1, x = setZ1)
```

## Zelig (3)

Plot:

```{r}
ci.plot(simZ1)
```


## <i class="fa fa-arrow-circle-o-up"></i> Seminar: modeling

<br>
<br>

Begin working on the statistical models for **your project**.

and/or

**Out of Lecture Challenge**: Estimate a normal regression model and **plot 
predicted values** across a range of fitted values. Bonus: do so with a measure
of uncertainty.

## References
